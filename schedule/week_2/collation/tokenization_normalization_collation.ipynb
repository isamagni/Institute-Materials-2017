{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding tokenization and normalization to collation\n",
    "\n",
    "## Starting point"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 1,
>>>>>>> faaca014276674eb75eebd03f46ce01d68ac1e3b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-------+---------------------+------+------+\n",
      "| A | The | quick | brown | fox jumped over the | lazy | dog. |\n",
      "| B | The | -     | brown | fox jumped over the | -    | dog. |\n",
      "| C | The | bad   | -     | fox jumped over the | lazy | dog. |\n",
      "+---+-----+-------+-------+---------------------+------+------+\n"
     ]
    }
   ],
   "source": [
    "from collatex import *\n",
    "collation = Collation()\n",
    "collation.add_plain_witness( \"A\", \"The quick brown fox jumped over the lazy dog.\")\n",
    "collation.add_plain_witness( \"B\", \"The brown fox jumped over the dog.\" )\n",
    "collation.add_plain_witness( \"C\", \"The bad fox jumped over the lazy dog.\" )\n",
    "table = collate(collation)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the text of the witness from its inclusion in the collation."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 27,
>>>>>>> faaca014276674eb75eebd03f46ce01d68ac1e3b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+-------+-------+---------------------+------+------+\n",
      "| A | The | quick | brown | fox jumped over the | lazy | dog. |\n",
      "| B | The | -     | brown | fox jumped over the | -    | dog. |\n",
      "| C | The | bad   | -     | fox jumped over the | lazy | dog. |\n",
      "+---+-----+-------+-------+---------------------+------+------+\n"
     ]
    }
   ],
   "source": [
    "collation = Collation()\n",
    "A_content = \"The quick brown fox jumped over the lazy dog.\"\n",
    "B_content = \"The brown fox jumped over the dog.\"\n",
    "C_content = \"The bad fox jumped over the lazy dog.\" \n",
    "collation.add_plain_witness( \"A\", A_content )\n",
    "collation.add_plain_witness( \"B\", B_content )\n",
    "collation.add_plain_witness( \"C\", C_content )\n",
    "table = collate(collation)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use functions to tokenize the witness text. Start with just one witness, and verify the result by outputting JSON."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 28,
>>>>>>> faaca014276674eb75eebd03f46ce01d68ac1e3b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'witnesses': [{'id': 'A', 'tokens': [{'t': 'The '}, {'t': 'quick '}, {'t': 'brown '}, {'t': 'fox '}, {'t': 'jumped '}, {'t': 'over '}, {'t': 'the '}, {'t': 'lazy '}, {'t': 'dog.'}]}]}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#importing the regualr expression module\n",
    "\n",
    "def tokenize(input):\n",
    "    return [create_token(token) for token in re.findall('\\S+\\s*',input)]\n",
    "\n",
    "#tokenize it through the re. I call each of the pieces token.\n",
    "\n",
    "def create_token(input):\n",
    "    return {\"t\": input}\n",
    "\n",
    "collation = Collation()\n",
    "A_content = \"The quick brown fox jumped over the lazy dog.\"\n",
    "B_content = \"The brown fox jumped over the dog.\"\n",
    "C_content = \"The bad fox jumped over the lazy dog.\" \n",
    "\n",
    "witness_list = []\n",
    "witness_list.append({\"id\": \"A\", \"tokens\": tokenize(A_content)})\n",
    "#print(witness_list)\n",
    "\n",
    "#dictionary has two keys: id and tokens\n",
    "\n",
    "json_input = {\"witnesses\": witness_list}\n",
    "print(json_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add simple normalization. This won’t affect the collation output, but we can verify that it’s working."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 29,
>>>>>>> faaca014276674eb75eebd03f46ce01d68ac1e3b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'witnesses': [{'id': 'A', 'tokens': [{'t': 'The ', 'n': 'the '}, {'t': 'quick ', 'n': 'quick '}, {'t': 'brown ', 'n': 'brown '}, {'t': 'fox ', 'n': 'fox '}, {'t': 'jumped ', 'n': 'jumped '}, {'t': 'over ', 'n': 'over '}, {'t': 'the ', 'n': 'the '}, {'t': 'lazy ', 'n': 'lazy '}, {'t': 'dog.', 'n': 'dog.'}]}, {'id': 'B', 'tokens': [{'t': 'The ', 'n': 'the '}, {'t': 'brown ', 'n': 'brown '}, {'t': 'fox ', 'n': 'fox '}, {'t': 'jumped ', 'n': 'jumped '}, {'t': 'over ', 'n': 'over '}, {'t': 'the ', 'n': 'the '}, {'t': 'dog.', 'n': 'dog.'}]}, {'id': 'C', 'tokens': [{'t': 'The ', 'n': 'the '}, {'t': 'bad ', 'n': 'bad '}, {'t': 'fox ', 'n': 'fox '}, {'t': 'jumped ', 'n': 'jumped '}, {'t': 'over ', 'n': 'over '}, {'t': 'the ', 'n': 'the '}, {'t': 'lazy ', 'n': 'lazy '}, {'t': 'dog.', 'n': 'dog.'}]}]}\n",
      "+---+-----+-------+-------+---------------------+------+------+\n",
      "| A | The | quick | brown | fox jumped over the | lazy | dog. |\n",
      "| B | The | -     | brown | fox jumped over the | -    | dog. |\n",
      "| C | The | bad   | -     | fox jumped over the | lazy | dog. |\n",
      "+---+-----+-------+-------+---------------------+------+------+\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize(input):\n",
    "    return input.lower()\n",
    "\n",
    "#new function normalize\n",
    "\n",
    "def tokenize(input):\n",
    "    return [create_token(token) for token in re.findall('\\S+\\s*',input)]\n",
    "\n",
    "def create_token(input):\n",
    "    return {\"t\": input, \"n\": normalize(input)}\n",
    "\n",
    "#so now instead of just returning t with input, I also return the n (normalized) input.\n",
    "\n",
    "collation = Collation()\n",
    "A_content = \"The quick brown fox jumped over the lazy dog.\"\n",
    "B_content = \"The brown fox jumped over the dog.\"\n",
    "C_content = \"The bad fox jumped over the lazy dog.\" \n",
    "\n",
    "witness_list = []\n",
    "witness_list.append({\"id\": \"A\", \"tokens\": tokenize(A_content)})\n",
    "witness_list.append({\"id\": \"B\", \"tokens\": tokenize(B_content)})\n",
    "witness_list.append({\"id\": \"C\", \"tokens\": tokenize(C_content)})\n",
    "\n",
    "json_input = {\"witnesses\": witness_list}\n",
    "print(json_input)\n",
    "table = collate(json_input)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the text to create a more complex example"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 10,
=======
   "execution_count": 30,
>>>>>>> faaca014276674eb75eebd03f46ce01d68ac1e3b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+---------+--------+\n",
      "| A | Look, a | gray | -       | koala! |\n",
      "| B | Look, a | big  | grey    | koala! |\n",
      "| C | Look, a | big  | wombat! | -      |\n",
      "+---+---------+------+---------+--------+\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize(input):\n",
    "    return input.lower()\n",
    "\n",
    "def tokenize(input):\n",
    "    return [create_token(token) for token in re.findall('\\S+\\s*',input)]\n",
    "\n",
    "def create_token(input):\n",
    "    return {\"t\": input, \"n\": normalize(input)}\n",
    "\n",
    "collation = Collation()\n",
    "A_content = \"Look, a gray koala!\"\n",
    "B_content = \"Look, a big grey koala!\"\n",
    "C_content = \"Look, a big wombat!\" \n",
    "\n",
    "#a set of three new witnesses\n",
    "\n",
    "witness_list = []\n",
    "witness_list.append({\"id\": \"A\", \"tokens\": tokenize(A_content)})\n",
    "witness_list.append({\"id\": \"B\", \"tokens\": tokenize(B_content)})\n",
    "witness_list.append({\"id\": \"C\", \"tokens\": tokenize(C_content)})\n",
    "\n",
    "json_input = {\"witnesses\": witness_list}\n",
    "# print(json_input)\n",
    "table = collate(json_input)\n",
    "print(table)\n",
    "\n",
    "#it's just doing string matching so it does not align wombat with koala, and gray with grey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enhance normalization to recognize that all animals are alike. (This introduces possible complications, which can be addressed through further enhancements.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+------+---------+\n",
      "| A | Look, a | gray | -    | koala!  |\n",
      "| B | Look, a | big  | grey | koala!  |\n",
      "| C | Look, a | big  | -    | wombat! |\n",
      "+---+---------+------+------+---------+\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def normalize(input):\n",
    "    input = re.sub('[' + string.punctuation + ']','',input)\n",
    "    animals = ['koala', 'wombat']\n",
    "    if input in animals:\n",
    "        return 'ANIMAL'\n",
    "    else:\n",
    "        return input.lower()\n",
    "    \n",
    "    #so, I solve that (just the wombat/koala) with a re (see re above).\n",
    "\n",
    "def tokenize(input):\n",
    "    return [create_token(token) for token in re.findall('\\S+\\s*',input)]\n",
    "\n",
    "def create_token(input):\n",
    "    return {\"t\": input, \"n\": normalize(input)}\n",
    "\n",
    "collation = Collation()\n",
    "A_content = \"Look, a gray koala!\"\n",
    "B_content = \"Look, a big grey koala!\"\n",
    "C_content = \"Look, a big wombat!\" \n",
    "\n",
    "witness_list = []\n",
    "witness_list.append({\"id\": \"A\", \"tokens\": tokenize(A_content)})\n",
    "witness_list.append({\"id\": \"B\", \"tokens\": tokenize(B_content)})\n",
    "witness_list.append({\"id\": \"C\", \"tokens\": tokenize(C_content)})\n",
    "\n",
    "json_input = {\"witnesses\": witness_list}\n",
    "# print(json_input)\n",
    "table = collate(json_input)\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The animals are now aligned, but the colors aren’t. We can address that through matching:"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
=======
   "execution_count": 32,
>>>>>>> faaca014276674eb75eebd03f46ce01d68ac1e3b
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+-----+------+---------+\n",
      "| A | Look, | a | -   | gray | koala!  |\n",
      "| B | Look, | a | big | grey | koala!  |\n",
      "| C | Look, | a | big | -    | wombat! |\n",
      "+---+-------+---+-----+------+---------+\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def normalize(input):\n",
    "    input = re.sub('[' + string.punctuation + ']','',input)\n",
    "    animals = ['koala', 'wombat']\n",
    "    if input in animals:\n",
    "        return 'ANIMAL'\n",
    "    else:\n",
    "        return input.lower()\n",
    "\n",
    "def tokenize(input):\n",
    "    return [create_token(token) for token in re.findall('\\S+\\s*',input)]\n",
    "\n",
    "def create_token(input):\n",
    "    return {\"t\": input, \"n\": normalize(input)}\n",
    "\n",
    "collation = Collation()\n",
    "A_content = \"Look, a gray koala!\"\n",
    "B_content = \"Look, a big grey koala!\"\n",
    "C_content = \"Look, a big wombat!\" \n",
    "\n",
    "witness_list = []\n",
    "witness_list.append({\"id\": \"A\", \"tokens\": tokenize(A_content)})\n",
    "witness_list.append({\"id\": \"B\", \"tokens\": tokenize(B_content)})\n",
    "witness_list.append({\"id\": \"C\", \"tokens\": tokenize(C_content)})\n",
    "\n",
    "json_input = {\"witnesses\": witness_list}\n",
    "# print(json_input)\n",
    "table = collate(json_input, near_match=True, segmentation=False)\n",
    "print(table)\n",
    "\n",
    "# now I solve the gray/grey with near matching. I say near_match=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
